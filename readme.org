* Content :TOC_3:
- [[#os2ds][os2ds]]
  - [[#data][data]]
  - [[#structure][structure]]
    - [[#engine][engine]]
  - [[#docker][docker]]
    - [[#init][init]]
    - [[#setup][setup]]
    - [[#useful-docker-commands][useful docker commands]]
    - [[#db][db]]
    - [[#network][network]]
    - [[#run-things][run things]]
    - [[#backup-volumes][backup volumes]]
    - [[#remove-volumes][remove volumes]]
    - [[#gunicorn][gunicorn]]
    - [[#inspect-docker-settings][inspect docker settings]]
    - [[#docker_overwriteyml][docker_overwrite.yml]]
  - [[#api][API]]
  - [[#app][app]]
    - [[#django][django]]
    - [[#admin][admin]]
    - [[#report][report]]
    - [[#multi-containers][multi containers]]
    - [[#translations][translations]]
  - [[#ldap][ldap]]
    - [[#ldap-structure][ldap structure]]
    - [[#connect-to-magenta-ad][connect to Magenta AD]]
    - [[#local-ldap-server][local ldap server]]
  - [[#promethues][promethues]]
  - [[#grafana][grafana]]
    - [[#magentas-grafana][magentas grafana]]
  - [[#gcloud][gcloud]]
  - [[#local-setup][local setup]]
    - [[#local-setup-without-docker][local setup without docker]]
    - [[#poetry][poetry]]
  - [[#debugging][debugging]]
    - [[#repl][repl]]
    - [[#stacktrace][stacktrace]]
  - [[#testing][testing]]
    - [[#local][local]]
    - [[#docker-1][docker]]
  - [[#code-snippets][code snippets/]]
    - [[#example-with-downloading-from-google-drive][example with downloading from google drive]]
  - [[#registered-convertershandlers][registered converters/handlers]]
- [[#random][random]]
  - [[#pipeline-projekt-oversigt][Pipeline projekt oversigt]]
  - [[#get-requirementstxt][get requirements.txt]]
  - [[#debuging-of-docker-container-not-app-debuging][debug'ing of docker container (not app debug'ing)]]
  - [[#users-uidgid][users uid/gid]]
  - [[#git-hooks][git hooks]]
    - [[#installation][Installation]]
- [[#magenta][magenta]]
  - [[#time-tracking][time tracking]]

* os2ds
** data
[[file:os2ds/data/vst-lokalplan-20200416.pdf][pdf der udpakker til ca 3.000 filer]], sikkert pga embedded vektor grafik

** structure
https://os2datascanner.readthedocs.io/en/latest/pages/overview.html
https://labs.docs.magenta.dk/decision-log/2020/os2datascanner-saas.html

OS2datascanner consists of the following core services:

- OS2datascanner admin web application: Django application used for defining and starting scans.
- OS2datascanner admin services: A number of services used for scheduling jobs, collecting information from the queue, etc.
- OS2datascanner report web application: Django application used for displaying scan results.
- OS2datascanner report services: A number of services used for collecting information from the queue, etc.
- OS2datascanner engine components: Python based workers used to process data in order to perform scans.

All of which are packaged as Docker containers, with automatic builds and releases using a continuous integration and delivery pipeline.

In addition, these backing services are used:

- PostgreSQL databases - one for each web application
- RabbitMQ for communication between services
- File storage for web application uploads
- Load balancing (reverse proxy)
- Transactional email service

*** engine
=engine-module= or scanner engine - also known as the Pipeline™...
- engine_explorer
- engine_processor
- engine_matcher
- engine_tagger
- engine_exporter


download [[https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/doc/pipeline-architecture.svg][pipeline-architecture.svg]], print it as pdf using the browser and crop it
#+begin_src sh
# wget https://git.magenta.dk/os2datascanner/os2datascanner/-/raw/development/doc/pipeline-architecture.svg
sudo apt-get install texlive-extra-utils
pdfcrop pipeline-architecture.pdf pipeline-architecture.pdf
#+end_src

** docker
*** init
Install docker-compose
: pipx install docker-compose

Start all docker containers
: docker-compose up -d

Interfaces - Admin/rabbitMQ/report/Prometheus/grafana/API/swagger UI
http://localhost:8020/
http://localhost:8030/
http://localhost:8040/
http://localhost:8050
http://localhost:8060
http://localhost:8070/
http://localhost:8075/

http://localhost:8070/openapi.yaml

Default user and password for grafana is =admin= & =admin= as from [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/docker-compose.yml#L200][docker-compose.yml]]

show listening ports
: sudo ss -tulpn

*** setup
https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/README.rst

Make dirs for static files writable.
#+begin_src sh
git clone git@git.magenta.dk:os2datascanner/os2datascanner.git
cd os2datascanner

chmod -R o+w src/os2datascanner/projects/report/locale
chmod -R o+w src/os2datascanner/projects/report/reportapp/migrations
chmod -R o+w src/os2datascanner/projects/admin/locale
#+end_src

Likewise install DS as ~editable~ using pip, meaning that pip will install a link to DS in =site-packages=. With this, it is not necessary to modify =PYTHONPATH=
#+begin_src sh
pip install -e .

> fd "os2datascanner" ~/.pyenv/versions
/home/paw/.pyenv/versions/3.6.12/envs/os2ds/lib/python3.6/site-packages/os2datascanner.egg-link
#+end_src


Se [[https://udvikler.docs.magenta.dk/docker/index.html][udviklerhåndbogen]] for flere kommandoer

Build the containers
#+begin_src sh
docker-compose up --build -d

docker-compose exec -e DJANGO_SUPERUSER_PASSWORD=test admin_application python manage.py createsuperuser --noinput --username test --email test@test.dk
docker-compose exec -e DJANGO_SUPERUSER_PASSWORD=test report_application python manage.py createsuperuser --noinput --username test --email test@test.dk
#+end_src

: docker logs --timestamps --follow os2datascanner_engine_worker_1

pass for rabbitMQ is in =dev-environment/rabbitmq.env=
#+begin_src sh
RABBITMQ_DEFAULT_USER=os2ds
RABBITMQ_DEFAULT_PASS=os2ds
#+end_src

From =django 3.0= we can use [[https://docs.djangoproject.com/en/3.0/ref/django-admin/#django-admin-createsuperuser][environment variables]]
#+begin_src sh
DJANGO_SUPERUSER_PASSWORD=test DJANGO_SUPERUSER_USERNAME=test DJANGO_SUPERUSER_EMAIL=test@test docker-compose exec admin_application python manage.py createsuperuser --noinput
#+end_src

**** ports
https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers
- queue (rabbitmq)
  - 5672:5672, default =RABBITMQ_NODE_PORT= variable. Main port
  - 8030:15672
- admin_application
  - depends on: db, admin_frontend, queue
  - 8020:5000
- report_application
  - depends on: db,m report_frontend, queue
  - 8040:5000
- prometheus
  - 8050:9090
- grafana
  - 8060:3000
- idp
  - 8080:8080

**** debug template for docker, using DAP
See templates
https://github.com/ztlevi/LSP-Debug/blob/master/dap-config.el

*** useful docker commands
#+begin_src sh
docker network inspect os2datascanner_default
docker-compose logs | bat
docker logs -f mycontainer
docker stop $(docker ps -aq)
docker rm $(docker ps -aq)
docker system prune --all
#+end_src

#+begin_src sh
# delete all DocumentReport's in report app
docker exec report_module python manage.py shell -c "from os2datascanner.projects.report.reportapp.models.documentreport_model import DocumentReport; DocumentReport.objects.all().delete()"
#+end_src

*** db
In the Dockerfile for the image, =ENTRYPOINT= is set to [[https://github.com/docker-library/postgres/blob/master/12/alpine/docker-entrypoint.sh#L307][docker-entrypoint.sh]],
which sources all files =docker-compose.yml= copies to the container path
=/docker-entrypoint-initdb.d/=

See the [[https://docs.docker.com/engine/reference/builder/#entrypoint][docs for entrypoint]]


**** pgAdmin4
Connect to =db= (or =127.0.0.1= if =db= is not mapped in =/etc/hosts=). Connect as superuser

#+begin_src conf
user=postgres
password=os2datascanner
#+end_src
as specified in [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/feature/35236_show_dead_links/dev-environment/db.env][db.env]]

Right click on table, select =View/Edit Data=

**** backup
The easiest is to dump directly to/from localhost
#+begin_src sh
# backup
docker exec -t os2datascanner_db_1 pg_dumpall -U postgres --clean > db_dump_`date +%d-%m-%Y"_"%H_%M_%S`.sql

# restore
cat your_dump.sql | docker exec -i os2datascanenr_db_1 psql -U postgres
#+end_src



Alternative, create the dump-file inside the container and copy it to localhost.
As per this [[https://stackoverflow.com/a/63934857][SO]]
Let =pg_dump= write to a file inside the Docker container, then copy that out to the host

Backup. Maybe include =-c/--clean= (clean (drop) database objects before recreating)
#+begin_src sh
pg_dumpall --globals-only -U postgres > /globals.sql
pg_dump -Fc -U postgres os2datascanner_report > /dbc_report.dump
pg_dump -Fc -U postgres os2datascanner_admin > /dbc_admin.dump
# or dump all (text mode)
pg_dumpall -U postgres --clean > /db.dump

# Run with docker
docker exec -ti os2datascanner_db_1 bash -c 'pg_dumpall -U postgres --clean > /db.dump'
docker cp os2datascanner_db_1:/db.dump db.dump
#+end_src

Restore
#+begin_src sh
psql -U postgres -f globals.sql
# if dumped with -Fc flag (Format custom/binary)
pg_restore -U postgres -c -d os2datascanner_report db_report.dump
pg_restore -U postgres -c -d os2datascanner_admin db_admin.dump

# otherwise, if dumped as text
psql -U postgres < db.dump

# run with docker
docker cp db.dump os2datascanner_db_1:/db.dump
docker exec -ti os2datascanner_db_1 bash -c 'psql -U postgres < /db.dump'

# OR
docker cp dbc_admin.dump os2datascanner_db_1:/
docker exec -ti os2datascanner_db_1 bash -c 'pg_restore -U postgres -c -d os2datascanner_admin dbc_admin.dump'
docker exec -ti os2datascanner_db_1 bash -c 'pg_restore -U postgres -c -d os2datascanner_report dbc_report.dump'
#+end_src

Drop DB
#+begin_src sh
dkc exec db bash -c "dropdb -U postgres os2datascanner_report"
dkc exec db bash -c "dropdb -U postgres os2datascanner_admin"
#+end_src

Various DB backup files can be found [[file:os2ds/data/db/][here]].

**** restoring db

#+begin_src sh
dkc exec db bash
cd /docker-entrypoint-initdb.d
su postgres
psql  <<ENDSQL
dropdb ${REPORT_DATABASE_NAME};
dropuser ${REPORT_DATABASE_USER};
CREATE DATABASE ${REPORT_DATABASE_NAME};
CREATE USER ${REPORT_DATABASE_USER} WITH ENCRYPTED PASSWORD '${REPORT_DATABASE_PASSWORD}';
GRANT ALL PRIVILEGES ON DATABASE ${REPORT_DATABASE_NAME} TO ${REPORT_DATABASE_USER};
ENDSQL

#+end_src

*** network
Inspect network and find the IPs of the containers
#+begin_src sh
docker network inspect os2datascanner_default | grep -B 3 '172.19.0'
#+end_src

Find connections to container
#+begin_src sh
docker exec -ti os2datascanner_db_1 /bin/bash

# install ss
apt update
apt install iproute2

ss -tupnl
ss -natu
#+end_src
Thus for the =db= container, we might find that =admin_collector= and =report_collector= is connected, preventing us from restoring the =db=

: docker stop os2datascanner_report_collector_1 os2datascanner_admin_collector_1

*** run things
The settings are copied into the containers from =./dev_enviroment/admin/dev-settings.toml=, =./dev_enviroment/admin/dev-settings.toml= and =./dev-environment/rabbitmq.env=

os2ds sends and receives messages to =RabbitMQ= using the =pika= module. The =.toml= settings are reads by [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/src/os2datascanner/utils/pika_settings.py#L17-24][pika-settings.py]] using the =OS2DS=s =env= variables. *NOTE:* =pike-settings.py= does not know which app(report, admin or engine) is being run and try to read the =env= in the following order and stops when it encounter the first set variable, assuming that it correspond to the app.
- =OS2DS_ADMIN_USER_CONFIG_PATH=
- =OS2DS_ENGINE_USER_CONFIG_PATH=
- =OS2DS_REPORT_USER_CONFIG_PATH=

With =db= and =queue= running in docker, we can start the other apps locally. First =queue= needs to be mapped to =localhost=
#+begin_src sh
# insert in 3'nd line in file (2i)
sudo sed -i "3i127.0.1.1\tqueue db" /etc/hosts
# and remove 3'nd line
sudo sed -i "3d" /etc/hosts
#+end_src
as per the =[amqp]= section in eg. [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/dev-environment/report/dev-settings.toml][./dev_enviroment/admin/dev-settings.toml]].
#+begin_src yaml
[amqp]
# Nested amqp settings are picked up by the common amqp utility module
AMQP_HOST = "queue"
AMQP_USER = "os2ds"
AMQP_PWD = "os2ds"
#+end_src

Start the different components(remember to unset unneeded =env='s), eg. by prepending with
: O2DS_ENGINE_USER_CONFIG_PATH=

#+begin_src sh
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml
OS2DS_ADMIN_USER_CONFIG_PATH=dev-environment/admin/dev-settings.toml
OS2DS_REPORT_USER_CONFIG_PATH=dev-environment/report/dev-settings.toml
#+end_src

**** report
Report module - before starting the webserver generate static files and translations as per [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/docker/docker-entrypoint-django.sh#L27-30][docker-entrypoint-django.sh]]
#+begin_src sh
export OS2DS_REPORT_USER_CONFIG_PATH=dev-environment/report/dev-settings.toml DJANGO_SETTINGS_MODULE=os2datascanner.projects.report.settings
python -m os2datascanner.projects.report.manage pipeline_collector

# before starting the server, generate static files and translates
python -m os2datascanner.projects.report.manage collectstatic --no-input --clear --dry-run
python -m os2datascanner.projects.report.manage compilemessages

python -m os2datascanner.projects.report.manage runserver 0.0.0.0:8040

GUNICORN_WORKERS=2 gunicorn --config docker/gunicorn-settings.py --reload os2datascanner.projects.report.wsgi
#+end_src

**** Admin
#+begin_src sh
export OS2DS_ADMIN_USER_CONFIG_PATH=dev-environment/admin/dev-settings.toml DJANGO_SETTINGS_MODULE=os2datascanner.projects.admin.settings
python -m os2datascanner.projects.admin.manage pipeline_collector

# before starting the server, generate static files and translates
python -m os2datascanner.projects.admin.manage collectstatic --no-input --clear --dry-run
python -m os2datascanner.projects.admin.manage compilemessages

python -m os2datascanner.projects.admin.manage runserver 0.0.0.0:8040

GUNICORN_WORKERS=2 gunicorn --config docker/gunicorn-settings.py --reload os2datascanner.projects.admin.wsgi
#+end_src

**** engine
See the [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/src/os2datascanner/engine2/pipeline/README.md][README.md]] for info about the stages and messages sent between them
#+begin_src sh
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage explorer --debug --enable-metric
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage processor --debug --enable-metric
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage matcher --debug --enable-metric
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage tagger --debug --enable-metric
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage exporter --debug --enable-metric
#+end_src
**** tmux

Start by stopping relevant containers
#+begin_src sh
docker-compose stop engine_worker engine_exporter engine_explorer admin_collector report_collector
#+end_src

[[file:os2ds/bin/os2_tmux.sh][hackish script to run engine in tmux.]]
*** backup volumes
The persistent data is stored in a mounted data volume.

#+begin_src sh
docker inspect os2datascanner_db_1
"Mounts": [
    {
        "Type": "volume",
        "Name": "os2datascanner_postgres-data",
        "Source": "/var/lib/docker/volumes/os2datascanner_postgres-data/_data",
        "Destination": "/var/lib/postgresql/data",
        "Driver": "local",
        "Mode": "rw",
        "RW": true,
        "Propagation": ""
    },
#+end_src

As per this [[https://stackoverflow.com/a/23778599][SO]]
#+begin_src sh
docker run --rm --volumes-from os2datascanner_db_1 -v $(pwd):/backup busybox tar cvf /backup/backup.tar /var/lib/postgresql/data
#+end_src
- =--rm=: remove the container when it exits
- =--volumes-from os2ds_db_1=: attach to the volumes shared by the os2ds_db_1 container
- =-v $(pwd):/backup=: bind mount the current directory into the container; to write the tar file to
- =busybox=: a small simpler image - good for quick maintenance
- =tar cvf /backup/backup.tar /var/lib/...=: creates an uncompressed tar file of all the files in the /var/lib... directory
Thus a =backup.tar= file is created in the current directory.

Restore -- Not complete
#+begin_src sh
# create a new data container
docker create -v /data --name DATA2 busybox true
# untar the backup files into the new container᾿s data volume
docker run --rm --volumes-from DATA2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar
#+end_src
*** remove volumes
remove all volumes
#+begin_src sh
dkc down -v
#+end_src

remove named volume,
#+begin_src sh
# Stop and remove container's using the target volume
docker-compose stop NAME_OF_CONTAINER

# We need the force flag, "-f", as the container is still bound to the volume
docker-compose rm -f NAME_OF_CONTAINER

# Next find your volume name in the following list
docker volume ls

# Finally remove the volume
docker volume rm VOLUME_NAME
#+end_src

*** gunicorn
Gunicorn is app server, taking with the web server and the python app. In general:

Nginx will face the outside world and receive an request. It will serve media files (images, CSS, etc) directly from the file system. However, it can't talk directly to Django applications; it needs something that will run the application, feed it requests from the web, and return responses.

That's Gunicorn's job. Gunicorn will create a Unix socket, and serve responses to nginx via the wsgi protocol - the socket passes data in both directions:
#+begin_quote
The outside world <-> Nginx <-> The socket <-> Gunicorn <-> Django
#+end_quote
(Note: nginx and gunicorn can talk using tcp connections as well, if they are on separate machines. But there is less overhead with a socket than a tcp connection.)

In a development setup we can use djangos web server directly with =python -m manage.py runserver 0.0.0.0:8000=, but in production this is not [[https://github.com/django/channels/issues/142#issuecomment-216051605][not recommended]]. For more see [[https://uwsgi-docs.readthedocs.io/en/latest/tutorials/Django_and_nginx.html#concept][this nginx+uwsgi tutorial]].

*** inspect docker settings

Useful commands for inspecting
#+begin_src sh
docker-compose ls
docker inspect ID
docker inspect --format='{{json .Config}}' ID | jq
docker ps -q | xargs docker inspect --format '{{.State.Pid}}, {{.ID}}, {{.Config.Image}}'
#+end_src
*** docker_overwrite.yml
Be sure to have at least =docker-compose= version =1.28=, which supports [[https://docs.docker.com/compose/profiles/][profiles]]. Link or copy the [[file:os2ds/docs/docker-compose.override.yml][docker-compose.override.yml]] file
: ln -s ~/git/magenta/os2ds/docs/docker-compose.override.yml ~/git/os2datascanner/

Start the individual engine modules using the =debug= profile and *REMEMBER* to stop the =engine_worker= container.
#+begin_src sh
docker-compose --profile debug up -d
docker-compose stop engine_worker
#+end_src

An example of =docker-compose.override.yml=, exposing the postgres port.
#+begin_src yaml
version: "3"
services:
    db:
      ports:
        - "5432:5432"
#+end_src
** API
[[https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/doc/api.rst][docs]], [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/src/os2datascanner/server/wsgi.py][src]] OR http://localhost:8070/openapi.yaml

We need to set the API token in =dev-environment/api/dev-settings.toml=. It is in the format of a [[https://swagger.io/docs/specification/authentication/bearer-authentication/][bearer authentication]] as a pre-shared-key(bpsk), and we just set
#+begin_src conf
[server]
token = "thisIsNotASecret"
#+end_src
Then we can interact with the API endpoints {=openapi.yaml=, =dummy/1=, =scan/1=}

The best way to test it, is to use swaggerUI
http://localhost:8075

Or with the CLI
#+begin_src sh
sudo apt install httpie

http localhost:8070/openapi.yaml -d
http POST localhost:8070/dummy/1 AUTHORIZATION:'Bearer thisIsNotASecret'
echo '{"rule":{"type":"regex","expression":"[Tt]est"},"source":{"type":"data","content":"VGhpcyBpcyBvbmx5IGEgdGVzdA==","mime":"text/plain"}}' | http
 post localhost:8070/scan/1 AUTHORIZATION:'Bearer thisIsNotASecret'
# OR
curl -X POST "http://localhost:8070/scan/1" -H  "accept: application/jsonl" -H  "Authorization: Bearer os2ds" -H  "Content-Type: application/json" -d '{"rule":{"type":"regex","expression":"[Tt]est"},"source":{"type":"data","content":"VGhpcyBpcyBvbmx5IGEgdGVzdA==","mime":"text/plain"}}'

echo '{"rule":{"type":"regex","expression":"[Mm]orten"},"source":{"type":"web","url":"https://www.magenta.dk"}}' | http post localhost:8070/scan/1 AUTHORIZATION:'Bearer thisIsNotASecret'
#+end_src

The content is =base64= encoded
#+begin_src sh
echo -n "This is only a test" | base64 -w 0
VGhpcyBpcyBvbmx5IGEgdGVzdA==

echo 'VGhpcyBpcyBvbmx5IGEgdGVzdA==' | base64 --decode
#+end_src

Another example
The API expects valid JSON which is using ="= and not ='=.
Also, we need escape backslash so =\= becomes =\\=. =\b= is a literal backspace and needs to be escaped.

#+begin_src sh
echo '{"rule":{"type":"regex", "expression": "\\b(\d{2}(?:\d{2})?[\s]?\d{2}[\s]?\d{2})(?:[\s\-/\.]|\s\-\s)?(\d{4})\\b"},"source":{"type":"data","content":"'$(base64 -w 0 < cpr_test.txt)'","mime":"text/plain"}}' | sed 's/\\/\\\\/g' | http post localhost:8070/scan/1 AUTHORIZATION:'Bearer thisIsNotASecret' | jq
#+end_src

It is possible to submit a ~configuration~ list, which is parsed into the ~ScanSpecMessage.configuration~ dict. This is only used in =processor.py=, where =skip_mime_types= is read and used as
#+begin_quote
A list of the MIME types for which text conversions should not be performed.
This is chiefly used to switch off computationally-expensive OCR conversions.

The last character can be the wildcard "*"; for example, "image/*".
#+end_quote
#+begin_src sh
echo '{"rule":{"type":"regex", "expression": "1310"},"source":{"type":"data","content":"'$(base64 -w 0  < ~/git/os2datascanner/src/os2datascanner/engine2/tests/data/ocr/good/cpr.jpg )'","mime":"image/jpeg"}, "configuration": {"skip_mime_types": ["image/*"]}}' | http post localhost:8070/scan/1 AUTHORIZATION:'Bearer thisIsNotASecret' | jq
#+end_src

Follow the logs
#+begin_src sh
docker-compose logs --follow api_server
#+end_src
** app
*** django

**** docs
For django 2.2
[[https://ccbv.co.uk/projects/django/2.2/][Classy Class-Based Views]]
[[https://docs.djangoproject.com/en/2.2/][docs.djangoproject.com]]

**** django_extensions
[[https://github.com/django-extensions/django-extensions][django_extensions]] is a collection of extensions providing extra functionality to the =manage.py= command.

To generate a map of the database like [[file:os2ds/docs/report_database.png][report_database.png]], =graphviz= is required
#+begin_src sh
sudo apt-get install graphviz graphviz-dev
pip install django_extensions pygraphviz
#+end_src

Then add =django_extensions= to =INSTALLED_APPS= list in =src/os2datascanner/projects/report/default-settings.toml=. If added to =dev-environment/report/dev-settings.toml=, the list in =default-settings.toml= will be overwritten.

Useful extension commands
#+begin_src sh
export OS2DS_REPORT_USER_CONFIG_PATH=~/git/os2datascanner/dev-environment/report/dev-settings.toml
./manage.py graph_models -a -o report_database.png
./manage.py show_urls
./manage.py shell_plus  # auto-import all models
./manage.py print_settings
#+end_src

See all available commands with
: ./manage.py

**** debug
How to use developer tools
https://developer.mozilla.org/en-US/docs/Tools/Migrating_from_Firebug

#+begin_src python
from django.http import HttpResponse

def default(request):
    return HttpResponse("Foo says %d" % ni, mimetype="text/plain")
#+end_src

Maybe useful
- https://github.com/jazzband/django-debug-toolbar
- https://github.com/edoburu/django-debugtools

***** template tags
#+begin_src python
@register.filter
def pdb(element):
    import pdb; pdb.set_trace()
    return element
#+end_src

**** migrations
migrate(apply) between database layouts
#+begin_src sh
./manage.py migrate os2datascanner_report 0025_documentreport_created_timestamp
./manage.py migrate os2datascanner_report
#+end_src

*** admin
[[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/src/os2datascanner/projects/admin/adminapp/models/scannerjobs/scanner_model.py][Scanner]] is the main class for scanner models. It's =run= method is responsible for submitting a message to the pipeline. The pipeline is given in [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/src/os2datascanner/projects/admin/default-settings.toml#L140-142][default-settings.toml]]
#+begin_src conf
AMQP_PIPELINE_TARGET = "os2ds_scan_specs"
AMQP_CONVERSION_TARGET = "os2ds_conversions"
AMQP_EVENTS_TARGET = "os2ds_events"
#+end_src
queue =scan_spec= is read by =explorer.py=.

A =scan_spec= message might look like
#+begin_src json
{
    "time": "2021-03-20T09:10:22-05:00",
    "user": "test",
    "scanner": {
        "pk": 1,
        "name": "danni magenta"
    },
    "destination": "pipeline_collector",
    "organisation": {
        "name": "paws org",
        "uuid": "aa1aa88a-f249-4487-a166-00c4ca816ca7"
    }
}
#+end_src

*** report
The docker container sets [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/docker/docker-entrypoint-django.sh][docker-entrypoint-django.sh]] as =ENTRYPOINT=.
*** multi containers
Open http://localhost:8020 and http://localhost:8040 (admin and report app) in different [[https://support.mozilla.org/en-US/kb/containers][Firefox multi-containers]].
This allows us to be logged in both instances.

Maybe useful extensions(check the first one)
https://addons.mozilla.org/en-US/firefox/addon/containerise
https://addons.mozilla.org/en-US/firefox/addon/containers-sync
*** translations
Edit =PO= files, part of [[https://www.gnu.org/software/gettext/manual/html_node/PO-Mode.html#PO-Mode][GNU gettext]]
#+begin_src sh
sudo apt install gettext-el
#+end_src

Switch to =emacs-mode-map= using =C-z=.

Set the =Source Context= by =S=. For the admin part, set it to
: ~/git/os2datascanner/src/os2datascanner/projects/admin/

**** translate javascript
See the following [[https://git.magenta.dk/os2datascanner/os2datascanner/-/commit/155ccef34e271e84336a09081ec2ad95e7c6f676][commit]]

In html template, have
#+begin_src js
{% block scripts %}
    {{ block.super }}
    <script type="text/javascript" src="/jsi18n/"></script>
{% endblock %}

// then somewhere in a {% block body %}, within <main class="wrapper">.
{% trans "Do you really want to delete the status object for scanner '%(scanner_name)s'?" as r_u_sure %}
<button
    type="submit"
    class="button button--small button--grey"
    onclick="return confirm(interpolate('{{ r_u_sure|escapejs }}', {'scanner_name': '{{ status.scanner.name|escapejs }}'}, true))"
    title="{% trans 'Delete' %}">
</button>
#+end_src

In =django.po= have (line number point to the line of ={% trans "" %}= above.)
#+begin_src
#: adminapp/templates/os2datascanner/scan_status.html:93
#, python-format
msgid ""
"Do you really want to delete the status object for scanner "
"'%%(scanner_name)s'?"
msgstr "Er du sikker på, at du vil slette status for scanner "
"»%%(scanner_name)s«?"
#+end_src
** ldap
*** ldap structure
- =DN= distinguished name. Describe the fully qualified path to an entry
- =RDN= relative distinguished name. Describe the partial path to the entry relative to another entry in the tree.
[[file:img/LDAP_Directory_Strucuture.gif]]

- =dc= domain component (root)
- =ou= organisational unit
- =cn= common name
- =sn= surname

Example, DN
=cn=John Doe, ou=People, dc=sun.com=
A =RDN= is a component of the =DN=
=cn=John Doe, ou=People= is a RDN relative to the root RDN =dc=sun.com=.

*** connect to Magenta AD
Install =tailscale= as described in [[https://labs.docs.magenta.dk/services/tailscale.html][labs doc]] and =rdp= tool =remmina=
: sudo apt install remmina

Get the ip of the ad-server
#+begin_src sh
> tailscale status
100.105.214.39  magenta-2-sal        pmo@         linux   -
100.69.90.85    ad-server            md@          windows idle, tx 578044 rx 1071204
#+end_src

Connect with =remmina= using
: user: srvdsstaging
: password from bitwarden.

*** local ldap server
**** installation
install utils
#+begin_src sh
apt-file search ldapadd
sudo apt install ldap-utils
#+end_src

**** create users
#+begin_src sh
echo -e "dn: ou=jumbo,dc=magenta,dc=test\nobjectClass: organizationalUnit\nou:jumbo\n" > ne2; for k in `seq 1 10`; do echo -e "dn: cn=Test User $k,ou=jumbo,dc=magenta,dc=test\nobjectClass: inetOrgPerson\ncn: Test User $k\nsn: Test User $k\nmail: test$k@magenta.test\n" >> ne2; done
ldapadd -v -D cn=admin,dc=magenta,dc=test -w testMAG -f ne2
#+end_src

See the stucture, using phpLDAPadmin http://localhost:8100
: user: cn=admin,dc=magenta,dc=test
: pass: testMAG


** promethues
Prometheus collects metrics from the engine modules, using a =http pull= model(server is scraping target). This means that each module, when started with the =--metric= flag, also starts a prometheus web server (=start_http_server(args.prometheus_port)= in [[https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/src/os2datascanner/engine2/pipeline/run_stage.py#L67-68][run_stage.py]]) and corresponding =prometheus_summary= decorator providing the metrics.

http://localhost:8050

Prometheus is configured in [[https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/dev-environment/prometheus.yml][prometheus.yml]]. See http://localhost:8050/targets for status of targets(as set in =prometheus.yml=). http://localhost:8050/graph allows to query the pull'ed data. Start by typing =os2= to get fuzzy matching or click =Classic UI/insert metric at cursor= to see all available metrics.

The [[https://github.com/prometheus/prometheus/blob/main/Dockerfile][prometheus Dockerfile]] is based on busybox, so attach to it like(no =/bin/bash/=)
: docker exec -ti os2datascanner_prometheus_1 /bin/sh

The data is stored persistent on the mounted volume.
: docker inspect os2datascanner_prometheus_1| jq

Login to a engine container, check the http server is listening and query it
#+begin_src sh
docker exec -ti os2datascanner_engine_explorer_1 /bin/bash
ss -tupnl
curl http://localhost:9091/metric
#+end_src

=ss= should output something like
: tcp	LISTEN	0	5	0.0.0.0:9091	0.0.0.0:*	users:(("python",pid=1,fd=3))
indicating that the webserver is listening on port =9091=.

For local machine metrics, there is `apt install prometheus-node-exporter` which is a prometheus exporter for kernel and related metrics.

[[https://wikitech.wikimedia.org/wiki/Prometheus][https://wikitech.wikimedia.org/wiki/Prometheus]] might contain useful information.

** grafana
Grafana is web app, showing the data series collected by =prometheus=. For at showcase of what grafana can do, see [[https://grafana.wikimedia.org/][https://grafana.wikimedia.org/]] and the [[https://wikitech.wikimedia.org/wiki/Prometheus#/media/File:Prometheus_single_server.][architecture overview.png]].

http://localhost:8060
user/pass: =admin/admin= as from [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/docker-compose.yml#L200][docker-compose.yml]]

Grafana connects to prometheus server on port =9090=, as per [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/dev-environment/grafana/datasources.yml#L8][datasources.yml]]. The dashboard is setup in [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/dev-environment/grafana/dashboards/engine.json#L163][engine.json.]]

[[https://wikitech.wikimedia.org/wiki/Grafana.wikimedia.org][https://wikitech.wikimedia.org/wiki/Grafana.wikimedia.org]] might contain useful information.

*** magentas grafana
https://magenta.grafana.net

[[https://magenta.grafana.net/explore?orgId=1&left=\["now-15m","now","grafanacloud-magenta-logs",{"exemplar":true,"expr":"{minion=~\"os2ds.*\"}"}\]][See logs collected from servers]]

** gcloud

Setup instructions
[[https://labs.docs.magenta.dk/google-cloud-platform/instance-ssh-access.html][SSH-access to Google Cloud Compute instances (servers)]]

#+begin_src sh
gcloud compute ssh --tunnel-through-iap test-webserver-with-dummy-data --zone=europe-west4-a
#+end_src


Copy files
#+begin_src sh
gcloud compute scp MYFILE --tunnel-through-iap test-webserver-with-dummy-data:~/ --zone=europe-west4-a
#+end_src

Or use as a proxy
#+begin_src sh
gcloud compute ssh --tunnel-through-iap test-webserver-with-dummy-data --zone=europe-west4-a -- -N -p 22 -D localhost:5000
curl https://api.ip2geo.pl/json/
curl --proxy socks5://localhost:5000 https://api.ip2geo.pl/json/
#+end_src

** local setup
*** local setup without docker
Install both system- and python packages

Run =install.sh= to install system- and python packages or run this
#+begin_src sh
fd -e txt . requirements/python-requirements -x pip install -r
#+end_src

Below is shown how to do it using =poetry= (another python =env= manager).

To get tab-completion in =ipython=, run
: pip install jedi==0.17.2
[[https://stackoverflow.com/a/65465682][ipython autocomplete does not work]]

Install debug tools
: pip install debugpy

**** run
different pipelines to run -- NOTE deprecated. use =pipeline.run_stage <stage>= instead
#+begin_src sh
python -m os2datascanner.engine2.pipeline.explorer
python -m os2datascanner.engine2.pipeline.processor
python -m os2datascanner.engine2.pipeline.matcher
python -m os2datascanner.engine2.pipeline.tagger
python -m os2datascanner.engine2.pipeline.exporter
#+end_src

**** export variables
Be careful with setting the =OS2DS= env's like this. See [[*run things][run things]]
#+begin_src sh
os2ds=~/git/os2datascanner
export OS2DS_ENGINE_USER_CONFIG_PATH="$os2ds/contrib/config/engine-module/user-settings.toml" PYTHONPATH="$os2ds/src"
python -m os2datascanner.engine2.pipeline.explorer
#+end_src

or automatic loading of env's from =.envrc= file
#+begin_src sh
apt install direnv
direnv allow
#+end_src

#+begin_src sh
cat > .envrc << EOF
# https://direnv.net/man/direnv-stdlib.1.html
root_dir=$(expand_path .)
# root_dir=~/git/os2datascanner
export OS2DS_ENGINE_USER_CONFIG_PATH="$root_dir/contrib/config/engine-module/user-settings.toml"
export PYTHONPATH="$root_dir/src:$PYTHONPATH"
EOF
#+end_src

Here is a alternative, non-automatic way
https://stackoverflow.com/a/30969768

*** poetry
[[https://python-poetry.org/][poetry]] is a another virt. env. manager for python.

#+begin_src sh
sudo apt install $(cat requirements/sys-requirements/sys-requirements-engine.txt | grep -E '^[^# ]' | xargs )

# create python env.
pyenv local 3.6.4
poetry init -n

# add -n 1 to xargs if it is important that only one line is given each time
cat requirements/python-requirements/requirements-common.in | grep -E '^[^-# ]' | xargs poetry add
cat requirements/python-requirements/requirements-engine.in | grep -E '^[^-# ]' | xargs poetry add

# dev
cat requirements/python-requirements/requirements-test.in | grep -E '^[^-# ]' | xargs poetry add -D
cat requirements/python-requirements/requirements-lint.in | grep -E '^[^-# ]' | xargs poetry add -D

# ptvsd is deprecated in favor of debugpy
# poetry add --dev ptvsd
pip install jedi==0.17.2

# start the env
poetry shell
#+end_src

[[https://stackoverflow.com/a/64672646][Import requirements.txt into poetry]]
** debugging
*** repl
home made =repl=, https://git.magenta.dk/os2datascanner/os2datascanner/-/tree/feature/43622_debug_signal
activated by sending =SIGUSR2= to the =<pid>=, ie
#+begin_src sh
root@big-brain:~# kill -USR2 25456; sleep 0.5; cat /proc/25456/root/tmp/tipd.out.* & cat > /proc/25456/root/tmp/tipd.in.*
[1] 28829
Python 3.6.13 (default, Feb 16 2021, 20:24:15)
[GCC 8.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
(InteractiveConsole)
>>> rh
<__main__.main.<locals>.GenericRunner object at 0x7f45c2d50b38>
#+end_src

Man kan komme ind i en containers filsystem-namespace igennem hostsystemets =/proc/<pid>/root= mappe.

An alternative could be [[https://github.com/ionelmc/python-manhole][python-manhole]]
*** stacktrace
A stacktrace is printed to `stderr` if pipeline components receive `SIGUSR1`. The
scan continues without interuption.

The components must be startet using `run_stage`

Running the engine locally,
#+begin_src sh
python -m os2datascanner.engine2.pipeline.run_stage worker
ps aux | grep os2datascanner
kill -USR1 <pid>
#+end_src

Running the engine in Docker, using the namespace sharing between localhost and docker
#+begin_src sh
docker top os2datascanner_engine_worker_1  # get the <pid> of the python process
kill -USR1 <pid>
docker logs os2datascanner_engine_worker_1
#+end_src
** testing
*** local
#+begin_src sh
dse python -m unittest discover -s src/os2datascanner/engine2/tests
dse python -m unittest os2datascanner.engine2.tests.integration.test_engine2_pipeline.Engine2PipelineTests
dsa ./manage.py test os2datascanner.projects.admin.core.tests.test_flags.ModelChoiceEnumTest
#+end_src

*** docker
#+begin_src sh
docker-compose run admin_application python -m django test os2datascanner.projects.admin.tests
docker-compose run engine_explorer python -m unittest discover -s /code/src/os2datascanner/engine2/tests
docker-compose run report_application python -m django test os2datascanner.projects.report.tests
#+end_src

** code snippets/

#+begin_src python
from os2datascanner import SourceManager
C.convert(FilesystemHandle.make_handle("/home/alec/Documents/ocr_this_if_you_dare.png").follow(sm), C.types.OutputType.Text)
#+end_src

*** example with downloading from google drive

NOTE: It is important to keep quotes around ="EOF"=, otherwise shell variables will be parsed.
Also, I could not do =import os2datascanner.engine2.conversions as C=. =import= was parsed.

=cat > test.py <<-"EOF"= will remove indentation, thus making it possible to format the cat-strings fro better readablility, if needed.


[[https://drive.google.com/file/d/1JTo0WAlpGDfJADN2Dbha2rrNjVMexDDn/view?usp=sharing][download from google drive]] or use wget
#+begin_src sh
wget -r "https://drive.google.com/uc?export=download&id=1JTo0WAlpGDfJADN2Dbha2rrNjVMexDDn" -O ~/Downloads/cpr-examples.ods

cat > test.py <<"EOF"
from pathlib import Path
from pprint import pformat
from os2datascanner.engine2.model.core import Source, SourceManager
from os2datascanner.engine2.model.file import FilesystemHandle
from os2datascanner.engine2.rules.cpr import CPRRule
from os2datascanner.engine2.conversions import convert

fpath = Path("~/Downloads/cpr-examples.ods").expanduser()
rule = CPRRule(modulus_11=False, ignore_irrelevant=False)
sm = SourceManager()

lrfs = Source.from_handle(FilesystemHandle.make_handle(fpath))
lrfh = list(lrfs.handles(sm))[0]
lrfr =lrfh.follow(sm)
representation = convert(lrfr, rule.operates_on).value
print(representation)

matches = list( rule.match(representation))
print(pformat(matches))
EOF
#+end_src

** registered converters/handlers
List registered converters/handlers.
New converters needs to added to their respective =__init__.py= file.

#+begin_src python
from pprint import pprint
from os2datascanner.engine2.conversions import registry
from os2datascanner.engine2.model.core import Source

# converts, used for converting content to the type required by a rule
converters = registry.__converters
# pprint(f"converters {converters}")

## two ways of creating Sources using registrered handlers
# handle points to a container(fx. zip or docx); reinterpret handle as new Source
Source.from_handle(h)
# uses the handle's mime-typ
mime = h.guess_type() or mime = h.compute_type()
# list all registered mime handlers
Source._Source__mime_handlers

# create Source from url
Source.from_url(url)
# used the url's scheme, ie.
scheme = url.split(:)
# list all registrered scheme-handlers
Source._Source__url_handlers
#+end_src

See also [[file:os2ds/src/examples/gzip_source_from_string.py][gzip_source_from_string.py]]
* random
** Pipeline projekt oversigt
https://docs.google.com/spreadsheets/d/1WylHagXFc2rXuB2qEfnPng4an78U49WiiP8lJWMCc2Y/edit#gid=281544569

** get requirements.txt
Brug pip-tools og en requirements.in. Den spytter en requirements.txt der fungere som lock file. Det er bagudkompatibelt med alting.

** debug'ing of docker container (not app debug'ing)
https://udvikler.docs.magenta.dk/docker/debugging.html
https://udvikler.docs.magenta.dk/docker/commands.html

** users uid/gid
https://git.magenta.dk/labs/salt-automation/-/blob/master/states/global/service_accounts.sls

** git hooks
https://udvikler.docs.magenta.dk/git/hooks.html

Vi vil meget gerne have Redmine-ticketnumre i vores githistorik for at gøre det lettere at tracke et linje kodes oprindelse.

Da det ikke bør være op til den enkelte udviklers hukommelse og nidkærhed at sikre, at dette altid sker, anbefales det at anvende et githook til at automatisere det ud fra branchnavnet. Altså vil det være tilstrækkeligt at give sin branch det rigtige navn, hvorefter et githook kan gøre resten af arbejdet.

Der er udviklet to hooks til formålet. Det ene kaldes, når man invokerer git commit uden argumenter, mens det andet kaldes efter man har skrevet en commitbesked, f.eks. vha. git commit -m "En besked uden ticketnummer".

*** Installation
De to hooks spænder ikke ben for hinanden og kan fint anvendes samtidigt. De kan enten installeres per projekt eller globalt.

Hooks kan installeres per projekt ved at kopiere ovenstående til en fil i .git/hooks/ uden endelse og gøre den eksekverbar.

Hooks kan installeres globalt med kommandoen git config --global core.hooksPath <sti til mappe med hooks>.
* magenta
** time tracking
https://git.magenta.dk/internal/personal-projects/dan/time-tracking
