* Content :TOC_3:
- [[#os2ds][os2ds]]
  - [[#features][features]]
    - [[#html-performance][html-performance]]
    - [[#swedish-cpr][Swedish CPR]]
    - [[#implementer-nave--og-adresseregler-34001][Implementer nave- og adresseregler, 34001]]
    - [[#externe-link-til-websource-35210][Externe link til WebSource, 35210]]
    - [[#håndtering-af-døde-links-35236][Håndtering af døde links, 35236]]
    - [[#grafana-alarm---overvågning-af-pipelinetrin-42002][Grafana alarm - overvågning af pipelinetrin, 42002]]
    - [[#cpr-nummer-forbedring-39173][CPR-nummer forbedring, 39173]]
    - [[#nlp][NLP]]
  - [[#data][data]]
  - [[#structure][structure]]
    - [[#engine][engine]]
  - [[#docker][docker]]
    - [[#setup][setup]]
    - [[#useful-docker-commands][useful docker commands]]
    - [[#db][db]]
    - [[#network][network]]
    - [[#run-things][run things]]
    - [[#backup-volumes][backup volumes]]
    - [[#gunicorn][gunicorn]]
    - [[#inspect-docker-settings][inspect docker settings]]
    - [[#docker_overwriteyml][docker_overwrite.yml]]
  - [[#api][API]]
  - [[#app][app]]
    - [[#django][django]]
    - [[#admin][admin]]
    - [[#report][report]]
    - [[#multi-containers][multi containers]]
    - [[#translations][translations]]
  - [[#promethues][promethues]]
  - [[#grafana][grafana]]
  - [[#gcloud][gcloud]]
  - [[#local-setup][local setup]]
    - [[#local-setup-without-docker][local setup without docker]]
    - [[#poetry][poetry]]
  - [[#debugging][debugging]]
    - [[#repl][repl]]
    - [[#stacktrace][stacktrace]]
  - [[#registered-convertershandlers][registered converters/handlers]]
- [[#random][random]]
  - [[#pipeline-projekt-oversigt][Pipeline projekt oversigt]]
  - [[#get-requirementstxt][get requirements.txt]]
  - [[#debuging-of-docker-container-not-app-debuging][debug'ing of docker container (not app debug'ing)]]
  - [[#users-uidgid][users uid/gid]]
  - [[#git-hooks][git hooks]]
    - [[#installation][Installation]]
- [[#magenta][magenta]]
  - [[#time-tracking][time tracking]]

* os2ds

** features
*** html-performance
https://redmine.magenta-aps.dk/issues/37547

Før:
bla. pfd-filer blev udpakket til individuelle sider. Derefter konverteret med =pdf2text=.
Se [[https://redmine.magenta-aps.dk/issues/38126][Undgå unødvendige html-konvertering]], hvor der skiftes fra =pdftohtml= til =pdftotext/pdfimages=
Det er pt. kun =pdf= der konverteres til text. Office filer konverteres stadig til html.

**** redmine related
https://redmine.magenta-aps.dk/issues/38126
https://redmine.magenta-aps.dk/issues/30749

**** test
See [[file:os2ds/src/html_conversion/test_html_conversion.py]]

Based on the two test, I am confident that using =lxml= is an improvement over =bs4=. We are not loosing any body text.
***** timing
5 conversions of [[file:os2ds/data/html_benchmark/data/html.html][html.html]]

|   bs4 |     lxml |
|-------+----------|
| 1.008 | 0.060507 |

***** compare output of the two parsers
The output from =bs4= does not preserve linebreaks, thus to compare, we use
=wdiff=: it's a front-end to diff which produces word-by-word comparisons.

#+begin_src sh
apt install wdiff colordiff
wdiff -n html_bs4.txt html_lxml.txt | colordiff | bat
# OR show only difference
wdiff -3 html_bs4.txt html_lxml.txt
#+end_src

From the output, it seems the only difference is in the unicode for =>/<=, etc. symbols
#+BEGIN_SRC text
======================================================================
 [-<font color="">-]
    {+<font color="">+}
======================================================================
 [-year > 0-] {+year > 0+}
======================================================================
 [-1 ≤ month ≤ 12-]
   {+1 ≤ month ≤ 12+}
======================================================================
 [-1 ≤ month ≤ 12,-] {+1 ≤ month ≤ 12,+}
======================================================================
 [-1 ≤ day ≤ maxday-]
   {+1 ≤ day ≤ maxday+}
======================================================================
 [-1 ≤ day ≤ maxday,-] {+1 ≤ day ≤ maxday,+}
======================================================================
 [-1 ≤ month ≤ 12-]
   {+1 ≤ month ≤ 12+}
======================================================================
 [-1 ≤ day ≤ maxday-]
   {+1 ≤ day ≤ maxday+}
======================================================================
 [-1 ≤ month ≤ 12,-] {+1 ≤ month ≤ 12,+}
======================================================================
 [-1 ≤ day ≤ maxday,-] {+1 ≤ day ≤ maxday,+}
======================================================================
 [-0 ≤ hour ≤ 23-]
   {+0 ≤ hour ≤ 23+}
======================================================================
 [-0 ≤ minute ≤ 59-]
   {+0 ≤ minute ≤ 59+}
======================================================================
 [-0 ≤ s ≤ 59-] {+0 ≤ s ≤ 59+}
======================================================================
 [-0 ≤ hour ≤ 23,-] {+0 ≤ hour ≤ 23,+}
======================================================================
 [-0 ≤ minute ≤ 59,-] {+0 ≤ minute ≤ 59,+}
======================================================================
 [-0 ≤ second < 60,-] {+0 ≤ second < 60,+}
======================================================================
#+end_src

fx. from line around 4000 in =text.html=
#+begin_src html
<li><p>If <var data-x="">month</var> is not a number in the range 1&nbsp;&le;&nbsp;<var
4791   │    data-x="">month</var>&nbsp;&le;&nbsp;12, then fail.</p></li>
#+end_src

*** Swedish CPR
https://redmine.magenta-aps.dk/issues/40876

either 10 or 12 digit, last digit is a checksum, ie very similar to danish cpr
format:
#+begin_src text
yymmdd-xxxx
yyyymmdd-xxxx
#+end_src

info
https://en.wikipedia.org/wiki/Personal_identity_number_(Sweden)
https://sv.wikipedia.org/wiki/Personnummer_i_Sverige

regex
https://regex101.com/r/OuIbMa/2
ie. we modify danish CPR to match either {yy} OR {yyyy}, as per this [[https://stackoverflow.com/a/8177150][SO]].
: "\b(\d{2}(?:\d{2})?[\s]?\d{2}[\s]?\d{2})(?:[\s\-/\.]|\s\-\s)?(\d{4})\b"

#+begin_src sh
echo '{"rule":{"type":"regex", "expression": "\\b(\d{2}(?:\d{2})?[\s]?\d{2}[\s]?\d{2})(?:[\s\-/\.]|\s\-\s)?(\d{4})\\b"},"source":{"type":"data","content":"'$(base64 -w 0 < cpr_test.txt)'","mime":"text/plain"}}' | sed 's/\\/\\\\/g' | http post localhost:8070/scan/1 AUTHORIZATION:'Bearer os2ds' | jq
#+end_src


**** test
Download the wiki page
#+begin_src sh
wget --mirror --convert-links https://sv.wikipedia.org/wiki/Personnummer_i_Sverige
tree
└── sv.wikipedia.org
   ├── robots.txt
   └── wiki
      └── Personnummer_i_Sverige

# Eller hvis samtlige filer skal ligge lokalt.
wget -E -H -k -K -p -e robots=off  https://sv.wikipedia.org/wiki/Personnummer_i_Sverige
#+end_src

*** Implementer nave- og adresseregler, 34001
https://redmine.magenta-aps.dk/issues/34001

[[https://git.magenta.dk/os2datascanner/os2datascanner/-/tree/feature/34001_name_and_address][Alec preliminary code]]
Which is a refactoring of the code from the old [[https://git.magenta.dk/os2datascanner/os2datascanner-prototypes/-/tree/new-datascanner/scrapy-webscanner/scanners/rules][webscanner]] (see =address.py= / =name.py=)

We encode all datafiles as =utf8=.

**** conversion to utf8
from =iso-8859-1= to utf8
: iconv -f ISO-8859-1 gadenavne.txt -t UTF-8 -o da_addresses.txt


From =ascii= (7-bit subset of utf8) with unicode characters (fx =\u00d8->ø=) to 'real' utf8.
=C-x C-e= is your friend here.
#+begin_src sh
#!/usr/bin/env bash
set -euo pipefail

for filename in *.jsonl; do
    [ -e "$filename" ] || continue
    cat "$filename" | jq > "$filename".tmp
    rm "$filename"
    mv "$filename"{.tmp,}
done
#+end_src

or in case somethings break and we need to rewrite the extension, fx. =test.jsonl.tmp= -> =test.json=.
see [[https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html][Shell parameter expansion]] and an [[https://stackoverflow.com/a/965069][SO example]]
#+begin_src sh
for file in *.tmp; do
    mv "$file" "${file%.*}"
done
#+end_src
or use [[https://github.com/sharkdp/fd][fd-find]]

Here is a regex using [[https://www.regular-expressions.info/lookaround.html][negative lookahead]]
#+begin_src sh
^(?!.*(jsonl)).*$
#+end_src


**** conversion to jsonl
https://jsonlines.org/
JSON Lines text format, also called newline-delimited JSON.

#+begin_src python
#!/usr/bin/env python3

import json

filename = 'da_addresses.txt'
with open(filename, 'r') as fin:
    fileout = filename.rsplit('.', 1)[0] + '.jsonl'
    with open(fileout, 'w') as fout:
        for line in fin:
            # strip to ensure \n is not part of the string sorrounded by ""
            json.dump(line.rstrip(), fout, ensure_ascii=False)
            fout.write('\n')
#+end_src

**** regex
[[https://stackoverflow.com/questions/22937618/reference-what-does-this-regex-mean/22944075][SO wiki on regex]] and info about [[https://www.regular-expressions.info/unicode.html#category][regex unicode categories]], fx. =\p{Lu}=: uppercase letter.
[[https://www.regular-expressions.info/modifiers.html][regex modifiers]], fx =(?i)= for ignore case.

Regex can be slow to fail as [[https://www.regular-expressions.info/catastrophic.html][this simple example]] shows.
[[https://www.regexbuddy.com/download.html][RegexBuddy]](windows app) can debug the regex step-by-step and maybe help to optimize.
https://1337x.to/torrent/4257525/RegexBuddy-v4-10-Crack-FTUApps/

**** test
For name regex
https://regex101.com/r/nT9wL5/8

For address regex
https://regex101.com/r/zJBsXw/9

*** Externe link til WebSource, 35210
https://redmine.magenta-aps.dk/issues/35210

*** Håndtering af døde links, 35236
https://redmine.magenta-aps.dk/issues/35236

See [[file:os2ds/src/dead_links/readme.org][dead_links readme.org]] for example of json messages.

*** Grafana alarm - overvågning af pipelinetrin, 42002
https://redmine.magenta-aps.dk/issues/42002

Vi mangler overvågning af de enkelte pipeline trin i scannermotoren.
- Hvis rabbitmq oplever timeout fra en af pipeline trinene.
- Hvis et pipelinetrin går i stå og ikke spiser flere beskeder fra en fyldt kø.

*** CPR-nummer forbedring, 39173
Udspringer af [[https://redmine.magenta-aps.dk/issues/39173][COOPs falske positive]]
Men vi bygger videre på
https://redmine.magenta-aps.dk/issues/39173

Forslag
- Er der specialtegn før eller efter
- Er delimiters balanceret
- Kommer der et tal før eller efter
- Er ord før eller efter ikke enten (alle små, stort begyndelsesbogstav eller alle caps),
  dvs "uSNChanged" bør give =probability=0=
- indeholder ord før =cpr=

- NLP(natural language processing)


Leverance:
En af det nævnte løsninger og bevis på at det virker efter hensigten.

Se [[file:os2ds/src/cpr_improvements/cpr_test.py][cpr-test.py]]
**** Try it
#+begin_src sh
echo "{'rule':{'type':'cpr'},'source':{'type':'data','content':'$(base64 -w 0 < cpr-examples.txt)','mime':'text/plain'}}" | tr \' \" | http post localhost:8070/scan/1 AUTHORIZATION:'Bearer os2ds' | jq
#+end_src
*** NLP

https://www.nltk.org/book/ch01.html
https://towardsdatascience.com/nlp-approaches-to-data-anonymization-1fb5bde6b929
**** microsft

Microsoft have [[https://github.com/microsoft/presidio][presidio]], a /Context aware, pluggable and customizable PII anonymization service for text and images./.
It uses a mix of [[https://github.com/microsoft/presidio/tree/main/presidio-analyzer/presidio_analyzer/predefined_recognizers][predefined regex]] and [[https://github.com/microsoft/presidio/tree/main/presidio-analyzer/presidio_analyzer/nlp_engine][NLP using spaCy]].


** data
[[file:os2ds/data/vst-lokalplan-20200416.pdf][pdf der udpakker til ca 3.000 filer]], sikkert pga embedded vektor grafik

** structure
https://os2datascanner.readthedocs.io/en/latest/pages/overview.html
https://labs.docs.magenta.dk/decision-log/2020/os2datascanner-saas.html

OS2datascanner consists of the following core services:

- OS2datascanner admin web application: Django application used for defining and starting scans.
- OS2datascanner admin services: A number of services used for scheduling jobs, collecting information from the queue, etc.
- OS2datascanner report web application: Django application used for displaying scan results.
- OS2datascanner report services: A number of services used for collecting information from the queue, etc.
- OS2datascanner engine components: Python based workers used to process data in order to perform scans.

All of which are packaged as Docker containers, with automatic builds and releases using a continuous integration and delivery pipeline.

In addition, these backing services are used:

- PostgreSQL databases - one for each web application
- RabbitMQ for communication between services
- File storage for web application uploads
- Load balancing (reverse proxy)
- Transactional email service

*** engine
=engine-module= or scanner engine - also known as the Pipeline™...
- engine_explorer
- engine_processor
- engine_matcher
- engine_tagger
- engine_exporter


download [[https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/doc/pipeline-architecture.svg][pipeline-architecture.svg]], print it as pdf using the browser and crop it
#+begin_src sh
# wget https://git.magenta.dk/os2datascanner/os2datascanner/-/raw/development/doc/pipeline-architecture.svg
sudo apt-get install texlive-extra-utils
pdfcrop pipeline-architecture.pdf pipeline-architecture.pdf
#+end_src

** docker
Install docker-compose
: pipx install docker-compose

Start all docker containers
: docker-compose up -d

Interfaces - Admin/rabbitMQ/report/Prometheus/grafana/API/swagger UI
http://localhost:8020/
http://localhost:8030/
http://localhost:8040/
http://localhost:8050
http://localhost:8060
http://localhost:8070/
http://localhost:8075/

http://localhost:8070/openapi.yaml

Default user and password for grafana is =admin= & =admin= as from [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/docker-compose.yml#L200][docker-compose.yml]]

show listening ports
: sudo ss -tulpn


*** setup
https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/README.rst

Make dirs for static files writable.
#+begin_src sh
git clone git@git.magenta.dk:os2datascanner/os2datascanner.git
cd os2datascanner

chmod -R o+w src/os2datascanner/projects/report/locale
chmod -R o+w src/os2datascanner/projects/report/reportapp/migrations
chmod -R o+w src/os2datascanner/projects/admin/locale
#+end_src


Se [[https://udvikler.docs.magenta.dk/docker/index.html][udviklerhåndbogen]] for flere kommandoer

Build the containers
#+begin_src sh
docker-compose up --build -d
docker-compose exec admin_application python manage.py createsuperuser --username test --email test@test
docker-compose exec report_application python manage.py createsuperuser --username test --email test@test
#+end_src

: docker logs --timestamps --follow os2datascanner_engine_worker_1

pass for rabbitMQ is in =dev-environment/rabbitmq.env=
#+begin_src sh
RABBITMQ_DEFAULT_USER=os2ds
RABBITMQ_DEFAULT_PASS=os2ds
#+end_src

From =django 3.0= we can use [[https://docs.djangoproject.com/en/3.0/ref/django-admin/#django-admin-createsuperuser][environment variables]]
#+begin_src sh
DJANGO_SUPERUSER_PASSWORD=test DJANGO_SUPERUSER_USERNAME=test DJANGO_SUPERUSER_EMAIL=test@test docker-compose exec admin_application python manage.py createsuperuser --noinput
#+end_src

**** ports
https://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers
- queue (rabbitmq)
  - 5672:5672, default =RABBITMQ_NODE_PORT= variable. Main port
  - 8030:15672
- admin_application
  - depends on: db, admin_frontend, queue
  - 8020:5000
- report_application
  - depends on: db,m report_frontend, queue
  - 8040:5000
- prometheus
  - 8050:9090
- grafana
  - 8060:3000
- idp
  - 8080:8080

**** debug template for docker, using DAP
See templates
https://github.com/ztlevi/LSP-Debug/blob/master/dap-config.el

*** useful docker commands
#+begin_src sh
docker network inspect os2datascanner_default
docker-compose logs | bat
docker logs -f mycontainer
docker stop $(docker ps -aq)
docker rm $(docker ps -aq)
docker system prune --all
#+end_src

#+begin_src sh
# delete all DocumentReport's in report app
docker exec report_module python manage.py shell -c "from os2datascanner.projects.report.reportapp.models.documentreport_model import DocumentReport; DocumentReport.objects.all().delete()"
#+end_src

*** db
In the Dockerfile for the image, =ENTRYPOINT= is set to [[https://github.com/docker-library/postgres/blob/master/12/alpine/docker-entrypoint.sh#L307][docker-entrypoint.sh]],
which sources all files =docker-compose.yml= copies to the container path
=/docker-entrypoint-initdb.d/=

See the [[https://docs.docker.com/engine/reference/builder/#entrypoint][docs for entrypoint]]


**** pgAdmin4
Connect to =db= (or =127.0.0.1= if =db= is not mapped in =/etc/hosts=). Connect as superuser

#+begin_src conf
user=postgres
password=os2datascanner
#+end_src
as specified in [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/feature/35236_show_dead_links/dev-environment/db.env][db.env]]

Right click on table, select =View/Edit Data=

**** backup
As per this [[https://stackoverflow.com/a/63934857][SO]]
Let =pg_dump= write to a file inside the Docker container, then copy that out to the host

Backup. Maybe include =-c/--clean= (clean (drop) database objects before recreating)
#+begin_src sh
pg_dumpall --globals-only -U postgres > /globals.sql
pg_dump -Fc -U postgres os2datascanner_report > /dbc_report.dump
pg_dump -Fc -U postgres os2datascanner_admin > /dbc_admin.dump
# or dump all (text mode)
pg_dumpall -U postgres --clean > /db.dump

# Run with docker
docker exec -ti os2datascanner_db_1 bash -c 'pg_dumpall -U postgres --clean > /db.dump'
docker cp os2datascanner_db_1:/db.dump db.dump
#+end_src

Restore
#+begin_src sh
psql -U postgres -f globals.sql
# if dumped with -Fc flag (Format custom/binary)
pg_restore -U postgres -c -d os2datascanner_report db_report.dump
pg_restore -U postgres -c -d os2datascanner_admin db_admin.dump

# otherwise, if dumped as text
psql -U postgres < db.dump

# run with docker
docker cp db.dump os2datascanner_db_1:/db.dump
docker exec -ti os2datascanner_db_1 bash -c 'psql -U postgres < /db.dump'

# OR
docker cp dbc_admin.dump os2datascanner_db_1:/
docker exec -ti os2datascanner_db_1 bash -c 'pg_restore -U postgres -c -d os2datascanner_admin dbc_admin.dump'
docker exec -ti os2datascanner_db_1 bash -c 'pg_restore -U postgres -c -d os2datascanner_report dbc_report.dump'
#+end_src

Drop DB
#+begin_src sh
dropdb -U postgres os2datascanner_report
dropdb -U postgres os2datascanner_admin
#+end_src

Various DB backup files can be found [[file:os2ds/data/db/][here]].

*** network
Inspect network and find the IPs of the containers
#+begin_src sh
docker network inspect os2datascanner_default | grep -B 3 '172.19.0'
#+end_src

Find connections to container
#+begin_src sh
docker exec -ti os2datascanner_db_1 /bin/bash

# install ss
apt update
apt install iproute2

ss -tupnl
ss -natu
#+end_src
Thus for the =db= container, we might find that =admin_collector= and =report_collector= is connected, preventing us from restoring the =db=

: docker stop os2datascanner_report_collector_1 os2datascanner_admin_collector_1

*** run things
The settings are copied into the containers from =./dev_enviroment/admin/dev-settings.toml=, =./dev_enviroment/admin/dev-settings.toml= and =./dev-environment/rabbitmq.env=

os2ds sends and receives messages to =RabbitMQ= using the =pika= module. The =.toml= settings are reads by [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/src/os2datascanner/utils/pika_settings.py#L17-24][pika-settings.py]] using the =OS2DS=s =env= variables. *NOTE:* =pike-settings.py= does not know which app(report, admin or engine) is being run and try to read the =env= in the following order and stops when it encounter the first set variable, assuming that it correspond to the app.
- =OS2DS_ADMIN_USER_CONFIG_PATH=
- =OS2DS_ENGINE_USER_CONFIG_PATH=
- =OS2DS_REPORT_USER_CONFIG_PATH=

With =db= and =queue= running in docker, we can start the other apps locally. First =queue= needs to be mapped to =localhost=
#+begin_src sh
# insert in 3'nd line in file (2i)
sudo sed -i "3i127.0.1.1\tqueue db" /etc/hosts
# and remove 3'nd line
sudo sed -i "3d" /etc/hosts
#+end_src
as per the =[amqp]= section in eg. [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/dev-environment/report/dev-settings.toml][./dev_enviroment/admin/dev-settings.toml]].
#+begin_src yaml
[amqp]
# Nested amqp settings are picked up by the common amqp utility module
AMQP_HOST = "queue"
AMQP_USER = "os2ds"
AMQP_PWD = "os2ds"
#+end_src

Start the different components(remember to unset unneeded =env='s), eg. by prepending with
: O2DS_ENGINE_USER_CONFIG_PATH=

#+begin_src sh
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml
OS2DS_ADMIN_USER_CONFIG_PATH=dev-environment/admin/dev-settings.toml
OS2DS_REPORT_USER_CONFIG_PATH=dev-environment/report/dev-settings.toml
#+end_src

**** report
Report module - before starting the webserver generate static files and translations as per [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/docker/docker-entrypoint-django.sh#L27-30][docker-entrypoint-django.sh]]
#+begin_src sh
export OS2DS_REPORT_USER_CONFIG_PATH=dev-environment/report/dev-settings.toml DJANGO_SETTINGS_MODULE=os2datascanner.projects.report.settings
python -m os2datascanner.projects.report.manage pipeline_collector

# before starting the server, generate static files and translates
python -m os2datascanner.projects.report.manage collectstatic --no-input --clear --dry-run
python -m os2datascanner.projects.report.manage compilemessages

python -m os2datascanner.projects.report.manage runserver 0.0.0.0:8040

GUNICORN_WORKERS=2 gunicorn --config docker/gunicorn-settings.py --reload os2datascanner.projects.report.wsgi
#+end_src

**** Admin
#+begin_src sh
export OS2DS_ADMIN_USER_CONFIG_PATH=dev-environment/admin/dev-settings.toml DJANGO_SETTINGS_MODULE=os2datascanner.projects.admin.settings
python -m os2datascanner.projects.admin.manage pipeline_collector

# before starting the server, generate static files and translates
python -m os2datascanner.projects.admin.manage collectstatic --no-input --clear --dry-run
python -m os2datascanner.projects.admin.manage compilemessages

python -m os2datascanner.projects.admin.manage runserver 0.0.0.0:8040

GUNICORN_WORKERS=2 gunicorn --config docker/gunicorn-settings.py --reload os2datascanner.projects.admin.wsgi
#+end_src

**** engine
See the [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/src/os2datascanner/engine2/pipeline/README.md][README.md]] for info about the stages and messages sent between them
#+begin_src sh
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage explorer --debug --enable-metric
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage processor --debug --enable-metric
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage matcher --debug --enable-metric
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage tagger --debug --enable-metric
OS2DS_ENGINE_USER_CONFIG_PATH=dev-environment/engine/dev-settings.toml  python -m os2datascanner.engine2.pipeline.run_stage exporter --debug --enable-metric
#+end_src
**** tmux

Start by stopping relevant containers
#+begin_src sh
docker-compose stop engine_worker engine_exporter engine_explorer admin_collector report_collector
#+end_src

[[file:os2ds/bin/os2_tmux.sh][hackish script to run engine in tmux.]]
*** backup volumes
The persistent data is stored in a mounted data volume.

#+begin_src sh
docker inspect os2datascanner_db_1
"Mounts": [
    {
        "Type": "volume",
        "Name": "os2datascanner_postgres-data",
        "Source": "/var/lib/docker/volumes/os2datascanner_postgres-data/_data",
        "Destination": "/var/lib/postgresql/data",
        "Driver": "local",
        "Mode": "rw",
        "RW": true,
        "Propagation": ""
    },
#+end_src

As per this [[https://stackoverflow.com/a/23778599][SO]]
#+begin_src sh
docker run --rm --volumes-from os2datascanner_db_1 -v $(pwd):/backup busybox tar cvf /backup/backup.tar /var/lib/postgresql/data
#+end_src
- =--rm=: remove the container when it exits
- =--volumes-from os2ds_db_1=: attach to the volumes shared by the os2ds_db_1 container
- =-v $(pwd):/backup=: bind mount the current directory into the container; to write the tar file to
- =busybox=: a small simpler image - good for quick maintenance
- =tar cvf /backup/backup.tar /var/lib/...=: creates an uncompressed tar file of all the files in the /var/lib... directory
Thus a =backup.tar= file is created in the current directory.

Restore -- Not complete
#+begin_src sh
# create a new data container
docker create -v /data --name DATA2 busybox true
# untar the backup files into the new container᾿s data volume
docker run --rm --volumes-from DATA2 -v $(pwd):/backup busybox tar xvf /backup/backup.tar
#+end_src

*** gunicorn
Gunicorn is app server, taking with the web server and the python app. In general:

Nginx will face the outside world and receive an request. It will serve media files (images, CSS, etc) directly from the file system. However, it can't talk directly to Django applications; it needs something that will run the application, feed it requests from the web, and return responses.

That's Gunicorn's job. Gunicorn will create a Unix socket, and serve responses to nginx via the wsgi protocol - the socket passes data in both directions:
#+begin_quote
The outside world <-> Nginx <-> The socket <-> Gunicorn <-> Django
#+end_quote
(Note: nginx and gunicorn can talk using tcp connections as well, if they are on separate machines. But there is less overhead with a socket than a tcp connection.)

In a development setup we can use djangos web server directly with =python -m manage.py runserver 0.0.0.0:8000=, but in production this is not [[https://github.com/django/channels/issues/142#issuecomment-216051605][not recommended]]. For more see [[https://uwsgi-docs.readthedocs.io/en/latest/tutorials/Django_and_nginx.html#concept][this nginx+uwsgi tutorial]].

*** inspect docker settings

Useful commands for inspecting
#+begin_src sh
docker-compose ls
docker inspect ID
docker inspect --format='{{json .Config}}' ID | jq
docker ps -q | xargs docker inspect --format '{{.State.Pid}}, {{.ID}}, {{.Config.Image}}'
#+end_src
*** docker_overwrite.yml
Be sure to have at least =docker-compose= version =1.28=, which supports [[https://docs.docker.com/compose/profiles/][profiles]]. Link or copy the [[file:os2ds/docs/docker-compose.override.yml][docker-compose.override.yml]] file
: ln -s ~/git/magenta/os2ds/docs/docker-compose.override.yml ~/git/os2datascanner/

Start the individual engine modules using the =debug= profile and *REMEMBER* to stop the =engine_worker= container.
#+begin_src sh
docker-compose --profile debug up -d
docker-compose stop engine_worker
#+end_src

An example of =docker-compose.override.yml=, exposing the postgres port.
#+begin_src yaml
version: "3"
services:
    db:
      ports:
        - "5432:5432"
#+end_src
** API
[[https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/doc/api.rst][docs]], [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/src/os2datascanner/server/wsgi.py][src]] OR http://localhost:8070/openapi.yaml

We need to set the API token in =dev-environment/api/dev-settings.toml=. It is in the format of a [[https://swagger.io/docs/specification/authentication/bearer-authentication/][bearer authentication]] as a pre-shared-key(bpsk), and we just set
#+begin_src conf
[server]
token = "os2ds"
#+end_src
Then we can interact with the API endpoints {=openapi.yaml=, =dummy/1=, =scan/1=}

The best way to test it, is to use swaggerUI
http://localhost:8075

Or with the CLI
#+begin_src sh
sudo apt install httpie

http localhost:8070/openapi.yaml -d
http POST localhost:8070/dummy/1 AUTHORIZATION:'Bearer os2ds'
echo '{"rule":{"type":"regex","expression":"[Tt]est"},"source":{"type":"data","content":"VGhpcyBpcyBvbmx5IGEgdGVzdA==","mime":"text/plain"}}' | http
 post localhost:8070/scan/1 AUTHORIZATION:'Bearer os2ds'
# OR
curl -X POST "http://localhost:8070/scan/1" -H  "accept: application/jsonl" -H  "Authorization: Bearer os2ds" -H  "Content-Type: application/json" -d '{"rule":{"type":"regex","expression":"[Tt]est"},"source":{"type":"data","content":"VGhpcyBpcyBvbmx5IGEgdGVzdA==","mime":"text/plain"}}'

echo '{"rule":{"type":"regex","expression":"[Mm]orten"},"source":{"type":"web","url":"https://www.magenta.dk"}}' | http post localhost:8070/scan/1 AUTHORIZATION:'Bearer os2ds'
#+end_src

The content is =base64= encoded
#+begin_src sh
echo -n "This is only a test" | base64 -w 0
VGhpcyBpcyBvbmx5IGEgdGVzdA==

echo 'VGhpcyBpcyBvbmx5IGEgdGVzdA==' | base64 --decode
#+end_src

Another example
The API expects valid JSON which is using ="= and not ='=.
Also, we need escape backslash so =\= becomes =\\=. =\b= is a literal backspace and needs to be escaped.

#+begin_src sh
echo '{"rule":{"type":"regex", "expression": "\\b(\d{2}(?:\d{2})?[\s]?\d{2}[\s]?\d{2})(?:[\s\-/\.]|\s\-\s)?(\d{4})\\b"},"source":{"type":"data","content":"'$(base64 -w 0 < cpr_test.txt)'","mime":"text/plain"}}' | sed 's/\\/\\\\/g' | http post localhost:8070/scan/1 AUTHORIZATION:'Bearer os2ds' | jq
#+end_src


Follow the logs
#+begin_src sh
docker-compose logs --follow api_server
#+end_src
** app
*** django

**** docs
For django 2.2
[[https://ccbv.co.uk/projects/django/2.2/][Classy Class-Based Views]]
[[https://docs.djangoproject.com/en/2.2/][docs.djangoproject.com]]

**** django_extensions
[[https://github.com/django-extensions/django-extensions][django_extensions]] is a collection of extensions providing extra functionality to the =manage.py= command.

To generate a map of the database like [[file:os2ds/docs/report_database.png][report_database.png]], =graphviz= is required
#+begin_src sh
sudo apt-get install graphviz graphviz-dev
pip install django_extensions pygraphviz
#+end_src

Then add =django_extensions= to =INSTALLED_APPS= list in =src/os2datascanner/projects/report/default-settings.toml=. If added to =dev-environment/report/dev-settings.toml=, the list in =default-settings.toml= will be overwritten.

Useful extension commands
#+begin_src sh
export OS2DS_REPORT_USER_CONFIG_PATH=~/git/os2datascanner/dev-environment/report/dev-settings.toml
./manage.py graph_models -a -o report_database.png
./manage.py show_urls
./manage.py shell_plus  # auto-import all models
./manage.py print_settings
#+end_src

See all available commands with
: ./manage.py

**** debug
How to use developer tools
https://developer.mozilla.org/en-US/docs/Tools/Migrating_from_Firebug

#+begin_src python
from django.http import HttpResponse

def default(request):
    return HttpResponse("Foo says %d" % ni, mimetype="text/plain")
#+end_src

Maybe useful
- https://github.com/jazzband/django-debug-toolbar
- https://github.com/edoburu/django-debugtools

***** template tags
#+begin_src python
@register.filter
def pdb(element):
    import pdb; pdb.set_trace()
    return element
#+end_src

**** migrations
migrate(apply) between database layouts
#+begin_src sh
./manage.py migrate os2datascanner_report 0025_documentreport_created_timestamp
./manage.py migrate os2datascanner_report
#+end_src

*** admin
[[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/src/os2datascanner/projects/admin/adminapp/models/scannerjobs/scanner_model.py][Scanner]] is the main class for scanner models. It's =run= method is responsible for submitting a message to the pipeline. The pipeline is given in [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/src/os2datascanner/projects/admin/default-settings.toml#L140-142][default-settings.toml]]
#+begin_src conf
AMQP_PIPELINE_TARGET = "os2ds_scan_specs"
AMQP_CONVERSION_TARGET = "os2ds_conversions"
AMQP_EVENTS_TARGET = "os2ds_events"
#+end_src
queue =scan_spec= is read by =explorer.py=.

A =scan_spec= message might look like
#+begin_src json
{
    "time": "2021-03-20T09:10:22-05:00",
    "user": "test",
    "scanner": {
        "pk": 1,
        "name": "danni magenta"
    },
    "destination": "pipeline_collector",
    "organisation": {
        "name": "paws org",
        "uuid": "aa1aa88a-f249-4487-a166-00c4ca816ca7"
    }
}
#+end_src

*** report
The docker container sets [[https://git.magenta.dk/os2datascanner/os2datascanner/tree/development/docker/docker-entrypoint-django.sh][docker-entrypoint-django.sh]] as =ENTRYPOINT=.
*** multi containers
Open http://localhost:8020 and http://localhost:8040 (admin and report app) in different [[https://support.mozilla.org/en-US/kb/containers][Firefox multi-containers]].
This allows us to be logged in both instances.

Maybe useful extensions(check the first one)
https://addons.mozilla.org/en-US/firefox/addon/containerise
https://addons.mozilla.org/en-US/firefox/addon/containers-sync
*** translations
Edit =PO= files, part of [[https://www.gnu.org/software/gettext/manual/html_node/PO-Mode.html#PO-Mode][GNU gettext]]
#+begin_src sh
sudo apt install gettext-el
#+end_src

Switch to =emacs-mode-map= using =C-z=.

Set the =Source Context= by =S=. For the admin part, set it to
: ~/git/os2datascanner/src/os2datascanner/projects/admin/
** promethues
Prometheus collects metrics from the engine modules, using a =http pull= model(server is scraping target). This means that each module, when started with the =--metric= flag, also starts a prometheus web server (=start_http_server(args.prometheus_port)= in [[https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/src/os2datascanner/engine2/pipeline/run_stage.py#L67-68][run_stage.py]]) and corresponding =prometheus_summary= decorator providing the metrics.

http://localhost:8050

Prometheus is configured in [[https://git.magenta.dk/os2datascanner/os2datascanner/-/blob/development/dev-environment/prometheus.yml][prometheus.yml]]. See http://localhost:8050/targets for status of targets(as set in =prometheus.yml=). http://localhost:8050/graph allows to query the pull'ed data. Start by typing =os2= to get fuzzy matching or click =Classic UI/insert metric at cursor= to see all available metrics.

The [[https://github.com/prometheus/prometheus/blob/main/Dockerfile][prometheus Dockerfile]] is based on busybox, so attach to it like(no =/bin/bash/=)
: docker exec -ti os2datascanner_prometheus_1 /bin/sh

The data is stored persistent on the mounted volume.
: docker inspect os2datascanner_prometheus_1| jq

Login to a engine container, check the http server is listening and query it
#+begin_src sh
docker exec -ti os2datascanner_engine_explorer_1 /bin/bash
ss -tupnl
curl http://localhost:9091/metric
#+end_src

=ss= should output something like
: tcp	LISTEN	0	5	0.0.0.0:9091	0.0.0.0:*	users:(("python",pid=1,fd=3))
indicating that the webserver is listening on port =9091=.

For local machine metrics, there is `apt install prometheus-node-exporter` which is a prometheus exporter for kernel and related metrics.

[[https://wikitech.wikimedia.org/wiki/Prometheus][https://wikitech.wikimedia.org/wiki/Prometheus]] might contain useful information.

** grafana
Grafana is web app, showing the data series collected by =prometheus=. For at showcase of what grafana can do, see [[https://grafana.wikimedia.org/][https://grafana.wikimedia.org/]] and the [[https://wikitech.wikimedia.org/wiki/Prometheus#/media/File:Prometheus_single_server.][architecture overview.png]].

http://localhost:8060
user/pass: =admin/admin= as from [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/docker-compose.yml#L200][docker-compose.yml]]

Grafana connects to prometheus server on port =9090=, as per [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/dev-environment/grafana/datasources.yml#L8][datasources.yml]]. The dashboard is setup in [[https://git.magenta.dk/os2datascanner/os2datascanner/blob/development/dev-environment/grafana/dashboards/engine.json#L163][engine.json.]]

[[https://wikitech.wikimedia.org/wiki/Grafana.wikimedia.org][https://wikitech.wikimedia.org/wiki/Grafana.wikimedia.org]] might contain useful information.
** gcloud

Setup instructions
[[https://labs.docs.magenta.dk/google-cloud-platform/instance-ssh-access.html][SSH-access to Google Cloud Compute instances (servers)]]

#+begin_src sh
gcloud compute ssh --tunnel-through-iap test-webserver-with-dummy-data --zone=europe-west4-a
#+end_src


Copy files
#+begin_src sh
gcloud compute scp MYFILE --tunnel-through-iap test-webserver-with-dummy-data:~/ --zone=europe-west4-a
#+end_src

Or use as a proxy
#+begin_src sh
gcloud compute ssh --tunnel-through-iap test-webserver-with-dummy-data --zone=europe-west4-a -- -N -p 22 -D localhost:5000
curl https://api.ip2geo.pl/json/
curl --proxy socks5://localhost:5000 https://api.ip2geo.pl/json/
#+end_src

** local setup
*** local setup without docker
Install both system- and python packages

Run =install.sh= to install system- and python packages or run this
#+begin_src sh
fd -e txt . requirements/python-requirements -x pip install -r
#+end_src

Below is shown how to do it using =poetry= (another python =env= manager).

To get tab-completion in =ipython=, run
: pip install jedi==0.17.2
[[https://stackoverflow.com/a/65465682][ipython autocomplete does not work]]

Install debug tools
: pip install debugpy

**** run
different pipelines to run -- NOTE deprecated. use =pipeline.run_stage <stage>= instead
#+begin_src sh
python -m os2datascanner.engine2.pipeline.explorer
python -m os2datascanner.engine2.pipeline.processor
python -m os2datascanner.engine2.pipeline.matcher
python -m os2datascanner.engine2.pipeline.tagger
python -m os2datascanner.engine2.pipeline.exporter
#+end_src

**** export variables
Be careful with setting the =OS2DS= env's like this. See [[*run things][run things]]
#+begin_src sh
os2ds=~/git/os2datascanner
export OS2DS_ENGINE_USER_CONFIG_PATH="$os2ds/contrib/config/engine-module/user-settings.toml" PYTHONPATH="$os2ds/src"
python -m os2datascanner.engine2.pipeline.explorer
#+end_src

or automatic loading of env's from =.envrc= file
#+begin_src sh
apt install direnv
direnv allow
#+end_src

#+begin_src sh
cat > .envrc << EOF
# https://direnv.net/man/direnv-stdlib.1.html
root_dir=$(expand_path .)
# root_dir=~/git/os2datascanner
export OS2DS_ENGINE_USER_CONFIG_PATH="$root_dir/contrib/config/engine-module/user-settings.toml"
export PYTHONPATH="$root_dir/src:$PYTHONPATH"
EOF
#+end_src

Here is a alternative, non-automatic way
https://stackoverflow.com/a/30969768

*** poetry
[[https://python-poetry.org/][poetry]] is a another virt. env. manager for python.

#+begin_src sh
sudo apt install $(cat requirements/sys-requirements/sys-requirements-engine.txt | grep -E '^[^# ]' | xargs )

# create python env.
pyenv local 3.6.4
poetry init -n

# add -n 1 to xargs if it is important that only one line is given each time
cat requirements/python-requirements/requirements-common.in | grep -E '^[^-# ]' | xargs poetry add
cat requirements/python-requirements/requirements-engine.in | grep -E '^[^-# ]' | xargs poetry add

# dev
cat requirements/python-requirements/requirements-test.in | grep -E '^[^-# ]' | xargs poetry add -D
cat requirements/python-requirements/requirements-lint.in | grep -E '^[^-# ]' | xargs poetry add -D

# ptvsd is deprecated in favor of debugpy
# poetry add --dev ptvsd
pip install jedi==0.17.2

# start the env
poetry shell
#+end_src

[[https://stackoverflow.com/a/64672646][Import requirements.txt into poetry]]
** debugging
*** repl
home made =repl=, https://git.magenta.dk/os2datascanner/os2datascanner/-/tree/feature/43622_debug_signal
activated by sending =SIGUSR2= to the =<pid>=, ie
#+begin_src sh
root@big-brain:~# kill -USR2 25456; sleep 0.5; cat /proc/25456/root/tmp/tipd.out.* & cat > /proc/25456/root/tmp/tipd.in.*
[1] 28829
Python 3.6.13 (default, Feb 16 2021, 20:24:15)
[GCC 8.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
(InteractiveConsole)
>>> rh
<__main__.main.<locals>.GenericRunner object at 0x7f45c2d50b38>
#+end_src

Man kan komme ind i en containers filsystem-namespace igennem hostsystemets =/proc/<pid>/root= mappe.

An alternative could be [[https://github.com/ionelmc/python-manhole][python-manhole]]
*** stacktrace
A stacktrace is printed to `stderr` if pipeline components receive `SIGUSR1`. The
scan continues without interuption.

The components must be startet using `run_stage`

Running the engine locally,
#+begin_src sh
python -m os2datascanner.engine2.pipeline.run_stage worker
ps aux | grep os2datascanner
kill -USR1 <pid>
#+end_src

Running the engine in Docker, using the namespace sharing between localhost and docker
#+begin_src sh
docker top os2datascanner_engine_worker_1  # get the <pid> of the python process
kill -USR1 <pid>
docker logs os2datascanner_engine_worker_1
#+end_src

** registered converters/handlers
List registered converters/handlers.
New converters needs to added to their respective =__init__.py= file.

#+begin_src python
from pprint import pprint
from os2datascanner.engine2.conversions import registry
from os2datascanner.engine2.model.core import Source

# converts, used for converting content to the type required by a rule
converters = registry.__converters
# pprint(f"converters {converters}")

## two ways of creating Sources using registrered handlers
# handle points to a container(fx. zip or docx); reinterpret handle as new Source
Source.from_handle(h)
# uses the handle's mime-typ
mime = h.guess_type() or mime = h.compute_type()
# list all registered mime handlers
Source._Source__mime_handlers

# create Source from url
Source.from_url(url)
# used the url's scheme, ie.
scheme = url.split(:)
# list all registrered scheme-handlers
Source._Source__url_handlers
#+end_src

See also [[file:os2ds/src/examples/gzip_source_from_string.py][gzip_source_from_string.py]]
* random
** Pipeline projekt oversigt
https://docs.google.com/spreadsheets/d/1WylHagXFc2rXuB2qEfnPng4an78U49WiiP8lJWMCc2Y/edit#gid=281544569

** get requirements.txt
Brug pip-tools og en requirements.in. Den spytter en requirements.txt der fungere som lock file. Det er bagudkompatibelt med alting.

** debug'ing of docker container (not app debug'ing)
https://udvikler.docs.magenta.dk/docker/debugging.html
https://udvikler.docs.magenta.dk/docker/commands.html

** users uid/gid
https://git.magenta.dk/labs/salt-automation/-/blob/master/states/global/service_accounts.sls

** git hooks
https://udvikler.docs.magenta.dk/git/hooks.html

Vi vil meget gerne have Redmine-ticketnumre i vores githistorik for at gøre det lettere at tracke et linje kodes oprindelse.

Da det ikke bør være op til den enkelte udviklers hukommelse og nidkærhed at sikre, at dette altid sker, anbefales det at anvende et githook til at automatisere det ud fra branchnavnet. Altså vil det være tilstrækkeligt at give sin branch det rigtige navn, hvorefter et githook kan gøre resten af arbejdet.

Der er udviklet to hooks til formålet. Det ene kaldes, når man invokerer git commit uden argumenter, mens det andet kaldes efter man har skrevet en commitbesked, f.eks. vha. git commit -m "En besked uden ticketnummer".

*** Installation
De to hooks spænder ikke ben for hinanden og kan fint anvendes samtidigt. De kan enten installeres per projekt eller globalt.

Hooks kan installeres per projekt ved at kopiere ovenstående til en fil i .git/hooks/ uden endelse og gøre den eksekverbar.

Hooks kan installeres globalt med kommandoen git config --global core.hooksPath <sti til mappe med hooks>.
* magenta
** time tracking
https://git.magenta.dk/internal/personal-projects/dan/time-tracking
